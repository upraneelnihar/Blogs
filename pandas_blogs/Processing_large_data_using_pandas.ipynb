{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips and Tricks to Process Large Data in Python Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As data scientists the first and foremost skill to have is the ability to be process and analyze data. Python pandas has been the most popular tool for data wrangling and data analysis. Usually pandas is really good and efficient in processing small data (usually from 100MB up to 1GB) where performance is rarely a concern.Once the data size increases we experience memory and perfromance issues. By understanding how pandas interprets data and by using some tricks we can efficiently process files large data.In this blog post we will see some of the common issues faced while processing large data in pandas and trick to overcome them along with a example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TL, DR:\n",
    "- By default Pandas interprets the numeric values as __`int64`__, __`float64`__ dtypes and string columns as __`object`__ dtypes which makes the dataframe to use a lot of memory.   \n",
    "- After analysing the data we can observe that incase of numeric columns not always we require 8 bytes of memory to store the data and by downcasting them to respective __`signed`__ and __`unsigned`__ values we can achieve a significant reduction in memory.\n",
    "- Incase of string columns by default pandas stores all the characters present in a string column, it doesnot identify mutiple occurences of same value. In order to achieve this we can convert the object column to a categorical column using inbuilt __`df.astype('category')`__. This gives us around 10 times of reduction in memory   ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the kaggle dataset for [Instacart market basket analysis](https://www.kaggle.com/c/instacart-market-basket-analysis). The dataset contains data for around 3lakh historical orders with multiple products per each order. Overall the dataset has size of 700MB with around 30 million rows and 9 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now load this data into a pandas dataframe and see how pandas interprets the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_aisles = pd.read_csv('Datasets/aisles.csv')\n",
    "df_departments = pd.read_csv('Datasets/departments.csv')\n",
    "df_order_prior = pd.read_csv('Datasets/order_products__prior.csv')\n",
    "df_order_train = pd.read_csv('Datasets/order_products__train.csv')\n",
    "df_orders = pd.read_csv('Datasets/orders.csv')\n",
    "df_products = pd.read_csv('Datasets/products.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aisles: \t (134, 2)\n",
      "departments: \t (21, 2)\n",
      "order_prior: \t (32434489, 4)\n",
      "order_train: \t (1384617, 4)\n",
      "orders: \t (3421083, 7)\n",
      "products: \t (49688, 4)\n"
     ]
    }
   ],
   "source": [
    "print('aisles: \\t',df_aisles.shape) \n",
    "print('departments: \\t',df_departments.shape) \n",
    "print('order_prior: \\t',df_order_prior.shape)\n",
    "print('order_train: \\t',df_order_train.shape)\n",
    "print('orders: \\t',df_orders.shape)\n",
    "print('products: \\t',df_products.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the historical orders data by merging all the dataframe.\n",
    "df_his_orders = (df_order_prior\n",
    "                 .merge(df_orders, on=['order_id'], how='left')\n",
    "                 .merge(df_products, on=['product_id'], how='left')\n",
    "                 .merge(df_aisles, on=['aisle_id'], how='left')\n",
    "                 .merge(df_departments, on=['department_id'], how='left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_aisles\n",
    "del df_departments\n",
    "del df_order_prior\n",
    "del df_order_train\n",
    "del df_orders\n",
    "del df_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32434489, 15)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_his_orders.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 32434489 entries, 0 to 32434488\n",
      "Data columns (total 15 columns):\n",
      "order_id                  int64\n",
      "product_id                int64\n",
      "add_to_cart_order         int64\n",
      "reordered                 int64\n",
      "user_id                   int64\n",
      "eval_set                  object\n",
      "order_number              int64\n",
      "order_dow                 int64\n",
      "order_hour_of_day         int64\n",
      "days_since_prior_order    float64\n",
      "product_name              object\n",
      "aisle_id                  int64\n",
      "department_id             int64\n",
      "aisle                     object\n",
      "department                object\n",
      "dtypes: float64(1), int64(10), object(4)\n",
      "memory usage: 11.4 GB\n"
     ]
    }
   ],
   "source": [
    "df_his_orders.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that by default pandas interprets all number columns as 64 bit integers, float columns `int64`,`float64` values and all string columns as `object` values. Each number value in the above dataframe has a size of 8bytes, and each string value has the size equal to sum of size taken for each character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us now see how to optimize the default pandas interpretation of various dtypes to reduce the dataframes size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Int64  & Float 64 Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_columns = [col for col in df_his_orders.columns if df_his_orders[col].dtype == 'int64']\n",
    "float_column = [col for col in df_his_orders.columns if df_his_orders[col].dtype == 'float64']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_id 2 3421083\n",
      "product_id 1 49688\n",
      "add_to_cart_order 1 145\n",
      "reordered 0 1\n",
      "user_id 1 206209\n",
      "order_number 1 99\n",
      "order_dow 0 6\n",
      "order_hour_of_day 0 23\n",
      "aisle_id 1 134\n",
      "department_id 1 21\n"
     ]
    }
   ],
   "source": [
    "# Printing Min and Max Values of `int_columns`\n",
    "for column in int_columns:\n",
    "    print(column, df_his_orders[column].min(),df_his_orders[column].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output we can see that none of the integer column really need a 64 bit integer to store the data w.r.t the min and max values of each column below are the actual byte sizes required by each column:\n",
    "\n",
    "- `order_id`, `product_id`, `user_id` : __`uint32`__ range(0 to 4294967295)\n",
    "- `add_to_cart_order`, `order_number`,`order_number`, `order_dow`, `order_hour_of_day`, `aisle_id`, `department_id` : __`uint16`__ range(0 to 65,535)\n",
    "\n",
    "__Note__: We are choosing unsigned integer as there are no negative values in the data\n",
    "\n",
    "Pandas has an inbulit method __`pd.to_numeric`__ to downcast a number to it's respective lower byte size. Sometimes we may face data loss issue while downcasting a numeric column to their respective `signed` or `unsigned` value, hence it is recommened that you analyse the data values as above before downcasting the values.\n",
    "\n",
    "Let us now doencast the integer columns and analyse the dataframe size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downcasting the numeric columns to respective unsigned integers\n",
    "for column in int_columns:\n",
    "    df_his_orders[column] = pd.to_numeric(df_his_orders[column], downcast='unsigned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "days_since_prior_order 0.0 30.0\n",
      "[8.0, 12.0, 7.0, 9.0, 30.0, 17.0, 5.0, 23.0, 10.0, 1.0, 3.0, 2.0, 13.0, 6.0, nan, 0.0, 25.0, 14.0, 18.0, 11.0, 21.0, 4.0, 15.0, 20.0, 19.0, 27.0, 16.0, 24.0, 22.0, 26.0, 28.0, 29.0]\n"
     ]
    }
   ],
   "source": [
    "# Float columns\n",
    "for column in float_column:\n",
    "    print(column, df_his_orders[column].min(),df_his_orders[column].max())\n",
    "\n",
    "print(list(df_his_orders['days_since_prior_order'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the float values are not actually floats but due to the missing value `nan` pandas is interpreting it as a float column. Let us fill the missing value with 0 for now and downcast the column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the ,missing value with 0 nand downcasting the data.\n",
    "df_his_orders['days_since_prior_order'].fillna(value=0, inplace=True)\n",
    "df_his_orders['days_since_prior_order'] = (pd\n",
    "                                           .to_numeric(df_his_orders['days_since_prior_order'], \n",
    "                                                       downcast='unsigned'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 32434489 entries, 0 to 32434488\n",
      "Data columns (total 15 columns):\n",
      "order_id                  uint32\n",
      "product_id                uint16\n",
      "add_to_cart_order         uint8\n",
      "reordered                 uint8\n",
      "user_id                   uint32\n",
      "eval_set                  object\n",
      "order_number              uint8\n",
      "order_dow                 uint8\n",
      "order_hour_of_day         uint8\n",
      "days_since_prior_order    uint8\n",
      "product_name              object\n",
      "aisle_id                  uint8\n",
      "department_id             uint8\n",
      "aisle                     object\n",
      "department                object\n",
      "dtypes: object(4), uint16(1), uint32(2), uint8(8)\n",
      "memory usage: 9.3 GB\n"
     ]
    }
   ],
   "source": [
    "# Anlaysing the memory usage of the dataframe after downcasting Int64 and Float64 values\n",
    "df_his_orders.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that by analysing the data values and downcasting them to their respective lower sizes we have achieved a 2GB decrease in the dataframe size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Object` columns are the columns which contribute to most of the size of a pandas dataframe. The size of each object column is equal to sum of size taken by all characters present in the column. \n",
    "\n",
    "Let us now analyse the data present in the `object` columns and see how we can optimize the size taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = [col for col in df_his_orders.columns if df_his_orders[col].dtype == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_set 1\n",
      "product_name 49677\n",
      "aisle 134\n",
      "department 21\n"
     ]
    }
   ],
   "source": [
    "# no of unique values present in each object columns\n",
    "for column in object_columns:\n",
    "    print(column, df_his_orders[column].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we can infer that each `object` column has only few unique values whihc are repeated over the rows and constituting for unneccesary memory storage. It would be useful if we can store this data once and re use it across the dataframe. In pandas we can achieve this by converting it to a categorical column using the pandas __`df[column_name].astype('category')`__.\n",
    "\n",
    "Note: __`df[column_name].astype`__ can also be used for converting integers and float columns from `int64` to their respective downcasted versions but the only problem is that you have to manually enter the size to which it need to downcast in  `astype` whereas incase if `pd.to_numeric` it infers by itself..   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the object columns to categorial columns\n",
    "for column in object_columns:\n",
    "    df_his_orders[column] = df_his_orders[column].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 32434489 entries, 0 to 32434488\n",
      "Data columns (total 15 columns):\n",
      "order_id                  uint32\n",
      "product_id                uint16\n",
      "add_to_cart_order         uint8\n",
      "reordered                 uint8\n",
      "user_id                   uint32\n",
      "eval_set                  category\n",
      "order_number              uint8\n",
      "order_dow                 uint8\n",
      "order_hour_of_day         uint8\n",
      "days_since_prior_order    uint8\n",
      "product_name              category\n",
      "aisle_id                  uint8\n",
      "department_id             uint8\n",
      "aisle                     category\n",
      "department                category\n",
      "dtypes: category(4), uint16(1), uint32(2), uint8(8)\n",
      "memory usage: 1.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Anlaysing the memory usage of the dataframe after converting object to category \n",
    "df_his_orders.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that by converting `object` columns to `categorical` columns we have achieved around 10 times reduction in the size of the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "By using some simple tricks listed below we can process large data in pandas efficiently.\n",
    "- Downcast numeric column values to their respective signed or unsigned values using __`pd.to_numeric`__ or __`df.astype`__.\n",
    "- Convert object column values to categorical values using __`df.astype('category')`__    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
